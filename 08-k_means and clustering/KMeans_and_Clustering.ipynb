{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bf07ef",
   "metadata": {},
   "source": [
    "\n",
    "# K-Means Clustering — Math, Implementation, and Visualization\n",
    "\n",
    "**Author:** Generated by ChatGPT  \n",
    "**Date:** 2025-10-09 13:22\n",
    "\n",
    "This notebook is a complete, practical guide to K-Means clustering. It covers:\n",
    "- The math behind K-Means (objective, derivations, convergence)\n",
    "- A from-scratch NumPy implementation (including k-means++)\n",
    "- Usage with `scikit-learn`\n",
    "- How to choose *k* (Elbow, Silhouette)\n",
    "- Preprocessing (scaling), dimensionality reduction (PCA) for visualization\n",
    "- Real and synthetic datasets (Iris, blobs, moons)\n",
    "- Strengths, limitations, and practical tips\n",
    "\n",
    "> All plots are made with **matplotlib** (no seaborn). Each chart is on its own figure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc1a1a6",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Math Background\n",
    "\n",
    "### 1.1 Problem Setup\n",
    "\n",
    "Given data points \\( X = \\{x_1, x_2, \\dots, x_n\\} \\subset \\mathbb{R}^d \\) and an integer \\(k\\), K-Means seeks cluster assignments \\(c_i \\in \\{1,\\dots,k\\}\\) and centroids \\( \\mu_1, \\dots, \\mu_k \\in \\mathbb{R}^d \\) that minimize the **within-cluster sum of squares (WCSS)**:\n",
    "\\[\n",
    "J(\\{\\mu_j\\}, \\{c_i\\}) \\;=\\; \\sum_{i=1}^{n} \\lVert x_i - \\mu_{c_i}\\rVert_2^2.\n",
    "\\]\n",
    "\n",
    "### 1.2 Alternating Minimization\n",
    "\n",
    "K-Means uses **coordinate descent** (a.k.a. Lloyd’s algorithm):\n",
    "\n",
    "1. **Assignment step (E-step):** with fixed centroids, assign each point to the nearest centroid  \n",
    "\\[\n",
    "c_i \\leftarrow \\arg\\min_{j \\in \\{1,\\dots,k\\}} \\lVert x_i - \\mu_j\\rVert_2^2.\n",
    "\\]\n",
    "2. **Update step (M-step):** with fixed assignments, update each centroid to the mean of its assigned points  \n",
    "\\[\n",
    "\\mu_j \\leftarrow \\frac{1}{|C_j|} \\sum_{i: c_i = j} x_i, \\quad C_j=\\{i \\mid c_i=j\\}.\n",
    "\\]\n",
    "\n",
    "**Derivation of the centroid update.** For fixed assignments, the objective decomposes across clusters:  \n",
    "\\[\n",
    "J = \\sum_{j=1}^k \\sum_{i \\in C_j} \\lVert x_i - \\mu_j \\rVert_2^2.\n",
    "\\]\n",
    "For a single cluster \\(j\\), minimize \\( f(\\mu_j) = \\sum_{i \\in C_j} \\lVert x_i - \\mu_j \\rVert_2^2 \\) w.r.t. \\(\\mu_j\\). Taking derivative and setting to zero:  \n",
    "\\[\n",
    "\\frac{\\partial f}{\\partial \\mu_j} = \\sum_{i \\in C_j} 2(\\mu_j - x_i) = 2\\left(|C_j|\\mu_j - \\sum_{i \\in C_j} x_i\\right)=0\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "\\mu_j = \\frac{1}{|C_j|}\\sum_{i\\in C_j} x_i.\n",
    "\\]\n",
    "\n",
    "Thus, the update is the **cluster mean**.\n",
    "\n",
    "### 1.3 Convergence & Complexity\n",
    "\n",
    "- Each iteration **does not increase** \\(J\\) and the algorithm converges to a **local minimum** (not necessarily the global minimum).  \n",
    "- Complexity per iteration is \\(O(nkd)\\): computing distances from \\(n\\) points to \\(k\\) centroids in \\(d\\) dimensions, plus \\(O(nd)\\) to recompute means.  \n",
    "- Initialization matters (to avoid poor local minima)—**k-means++** is a standard choice.\n",
    "\n",
    "### 1.4 Choosing \\(k\\)\n",
    "\n",
    "- **Elbow method:** plot \\(J(k)\\) vs. \\(k\\) and look for an “elbow.”  \n",
    "- **Silhouette score:** for sample \\(i\\), let \\(a(i)\\) be the mean intra-cluster distance and \\(b(i)\\) the mean nearest-cluster distance; the silhouette is  \n",
    "\\[\n",
    "s(i) = \\frac{b(i)-a(i)}{\\max\\{a(i), b(i)\\}} \\in [-1, 1].\n",
    "\\]\n",
    "Higher is better; average over all points.\n",
    "\n",
    "### 1.5 When K-Means Works / Fails\n",
    "\n",
    "- Works best when clusters are **spherical**, **similar size**, and **similar density**.  \n",
    "- Can fail on **non-convex** shapes (e.g., moons) or **varying density/size** clusters. Dimensionality reduction, feature scaling, or using other clustering methods may help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350f792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports used across the notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Versions: numpy\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1924046a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. From-Scratch K-Means (NumPy)\n",
    "\n",
    "Below is a simple, educational implementation of K-Means. It supports:\n",
    "- random initialization or k-means++\n",
    "- maximum iterations and tolerance-based early stopping\n",
    "- returning inertia (WCSS) and labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pairwise_sq_dists(X, C):\n",
    "    \"\"\"\n",
    "    Compute squared Euclidean distances between each row of X (n,d)\n",
    "    and each row of C (k,d). Returns (n,k).\n",
    "    \"\"\"\n",
    "    # ||x - c||^2 = ||x||^2 + ||c||^2 - 2 x.c\n",
    "    X_norm = np.sum(X**2, axis=1, keepdims=True)     # (n,1)\n",
    "    C_norm = np.sum(C**2, axis=1, keepdims=True).T   # (1,k)\n",
    "    return X_norm + C_norm - 2 * X @ C.T\n",
    "\n",
    "def init_centroids_random(X, k, rng=None):\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    idx = rng.choice(X.shape[0], size=k, replace=False)\n",
    "    return X[idx].copy()\n",
    "\n",
    "def init_centroids_kmeans_plus_plus(X, k, rng=None):\n",
    "    \"\"\"\n",
    "    Probabilistic initialization:\n",
    "    - Pick first centroid at random\n",
    "    - Next centroids chosen with probability proportional to D(x)^2 (distance to nearest chosen centroid)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(None if rng is None else rng)\n",
    "    n = X.shape[0]\n",
    "    centroids = []\n",
    "    # Choose first centroid randomly\n",
    "    first = rng.integers(0, n)\n",
    "    centroids.append(X[first])\n",
    "    # Choose remaining\n",
    "    for _ in range(1, k):\n",
    "        d2 = np.min(pairwise_sq_dists(X, np.array(centroids)), axis=1)\n",
    "        probs = d2 / np.sum(d2)\n",
    "        next_idx = rng.choice(n, p=probs)\n",
    "        centroids.append(X[next_idx])\n",
    "    return np.array(centroids)\n",
    "\n",
    "def kmeans_numpy(X, k, init=\"k-means++\", max_iter=300, tol=1e-4, rng=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Simple K-Means using NumPy.\n",
    "    Returns: centroids (k,d), labels (n,), inertia (float), n_iter (int)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    if init == \"k-means++\":\n",
    "        C = init_centroids_kmeans_plus_plus(X, k, rng=rng)\n",
    "    elif init == \"random\":\n",
    "        C = init_centroids_random(X, k, rng=rng)\n",
    "    else:\n",
    "        raise ValueError(\"init must be 'k-means++' or 'random'\")\n",
    "\n",
    "    prev_inertia = None\n",
    "    for it in range(1, max_iter+1):\n",
    "        # E-step: assign\n",
    "        dists = pairwise_sq_dists(X, C)  # (n,k)\n",
    "        labels = np.argmin(dists, axis=1)\n",
    "\n",
    "        # M-step: update\n",
    "        new_C = np.zeros_like(C)\n",
    "        for j in range(k):\n",
    "            mask = labels == j\n",
    "            if np.any(mask):\n",
    "                new_C[j] = X[mask].mean(axis=0)\n",
    "            else:\n",
    "                # Handle empty cluster: reinitialize to a random point\n",
    "                new_C[j] = X[np.random.randint(0, X.shape[0])]\n",
    "\n",
    "        # Check convergence\n",
    "        inertia = np.sum((X - new_C[labels])**2)\n",
    "        if verbose:\n",
    "            print(f\"Iter {it:3d} | inertia = {inertia:.4f}\")\n",
    "        if prev_inertia is not None and abs(prev_inertia - inertia) <= tol * prev_inertia:\n",
    "            C = new_C\n",
    "            break\n",
    "\n",
    "        C = new_C\n",
    "        prev_inertia = inertia\n",
    "\n",
    "    return C, labels, inertia, it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43219aba",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Synthetic Data Demo (Blobs)\n",
    "\n",
    "We create 2D Gaussian blobs and run both our NumPy K-Means and `scikit-learn`'s KMeans. We visualize the clusters and centroids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d77847",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create synthetic blobs\n",
    "X, y_true = make_blobs(n_samples=600, centers=4, cluster_std=1.10, random_state=42)\n",
    "\n",
    "# Run our NumPy K-Means\n",
    "C_np, labels_np, inertia_np, n_iter_np = kmeans_numpy(X, k=4, init=\"k-means++\", max_iter=200, tol=1e-4, rng=42)\n",
    "print(\"NumPy K-Means -> inertia:\", inertia_np, \"iterations:\", n_iter_np)\n",
    "\n",
    "# Plot result\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], s=12)\n",
    "plt.scatter(C_np[:,0], C_np[:,1], marker=\"X\", s=200)\n",
    "plt.title(\"From-Scratch K-Means (NumPy) — Clusters and Centroids\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "# Compare with scikit-learn\n",
    "km = KMeans(n_clusters=4, n_init=10, random_state=42, init=\"k-means++\")\n",
    "km.fit(X)\n",
    "print(\"sklearn KMeans -> inertia:\", km.inertia_, \"iterations:\", km.n_iter_)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X[:,0], X[:,1], s=12)\n",
    "plt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:,1], marker=\"X\", s=200)\n",
    "plt.title(\"scikit-learn KMeans — Clusters and Centroids\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e01e12",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Choosing \\(k\\): Elbow and Silhouette\n",
    "\n",
    "We compute WCSS (inertia) for a range of \\(k\\) values to plot an elbow curve, and compute the average silhouette score to gauge cluster separation quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed494156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Ks = range(2, 11)\n",
    "inertias = []\n",
    "sil_scores = []\n",
    "\n",
    "for k in Ks:\n",
    "    km = KMeans(n_clusters=k, n_init=10, random_state=42, init=\"k-means++\")\n",
    "    labels = km.fit_predict(X)\n",
    "    inertias.append(km.inertia_)\n",
    "    sil = silhouette_score(X, labels)\n",
    "    sil_scores.append(sil)\n",
    "\n",
    "# Elbow plot\n",
    "plt.figure()\n",
    "plt.plot(list(Ks), inertias, marker=\"o\")\n",
    "plt.title(\"Elbow Method (WCSS vs k)\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"WCSS (Inertia)\")\n",
    "plt.show()\n",
    "\n",
    "# Silhouette plot\n",
    "plt.figure()\n",
    "plt.plot(list(Ks), sil_scores, marker=\"o\")\n",
    "plt.title(\"Average Silhouette Score vs k\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Silhouette Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f6b98",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Preprocessing & 2D Visualization via PCA\n",
    "\n",
    "Clustering often benefits from **feature scaling**. Here we scale features with `StandardScaler` and then reduce to 2D using `PCA` for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04170fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "km = KMeans(n_clusters=4, n_init=10, random_state=42)\n",
    "labels = km.fit_predict(X_pca)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_pca[:,0], X_pca[:,1], s=12)\n",
    "plt.title(\"PCA-Reduced Data Colored by K-Means Labels\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c2ff4",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Real Dataset: Iris\n",
    "\n",
    "We cluster the classic Iris dataset into \\(k=3\\) clusters and visualize in PCA space, then compare cluster labels with true species (for reference only; K-Means is unsupervised).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "species = iris.target\n",
    "\n",
    "scaler_iris = StandardScaler()\n",
    "X_iris_scaled = scaler_iris.fit_transform(X_iris)\n",
    "\n",
    "pca_iris = PCA(n_components=2, random_state=42)\n",
    "X_iris_pca = pca_iris.fit_transform(X_iris_scaled)\n",
    "\n",
    "km_iris = KMeans(n_clusters=3, n_init=20, random_state=42, init=\"k-means++\")\n",
    "labels_iris = km_iris.fit_predict(X_iris_scaled)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_iris_pca[:,0], X_iris_pca[:,1], s=20)\n",
    "plt.title(\"Iris — PCA Projection with K-Means Labels\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "\n",
    "# Quick evaluation via silhouette (unsupervised)\n",
    "sil_iris = silhouette_score(X_iris_scaled, labels_iris)\n",
    "print(\"Iris silhouette score (k=3):\", round(sil_iris, 3))\n",
    "\n",
    "# Optional: contingency table (unsupervised sanity-check)\n",
    "import pandas as pd\n",
    "df_ct = pd.crosstab(pd.Series(species, name=\"True\"), pd.Series(labels_iris, name=\"Cluster\"))\n",
    "df_ct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381bbc44",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Failure Case: Non-Convex Shapes (Two Moons)\n",
    "\n",
    "K-Means assumes roughly spherical clusters. On a **two moons** dataset, it struggles to separate the shapes, unlike density-based alternatives. This section demonstrates the limitation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_moons, y_moons = make_moons(n_samples=600, noise=0.08, random_state=42)\n",
    "\n",
    "km_moons = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
    "labels_moons = km_moons.fit_predict(X_moons)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(X_moons[:,0], X_moons[:,1], s=12)\n",
    "plt.title(\"Two Moons — K-Means Result (Often Suboptimal)\")\n",
    "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Silhouette score (moons, k=2):\", round(silhouette_score(X_moons, labels_moons), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396b3637",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Practical Tips\n",
    "\n",
    "- **Scale features** when dimensions have different units/magnitudes.  \n",
    "- Use **k-means++** initialization and **multiple restarts** (`n_init`) to reduce sensitivity to initialization.  \n",
    "- Run **Elbow** and **Silhouette** to guide your choice of \\(k\\).  \n",
    "- Consider **PCA** (or other DR) to mitigate noise and to visualize high-dimensional data.  \n",
    "- Beware of **outliers**: K-Means uses squared distances, so outliers can distort centroids.  \n",
    "- If your data has **non-convex** or **varying-density** clusters, consider DBSCAN or spectral clustering instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15beea2",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Appendix: K-Means Pseudocode\n",
    "\n",
    "```\n",
    "Input: data X ∈ ℝ^{n×d}, number of clusters k\n",
    "Initialize centroids μ₁,…,μ_k  (e.g., k-means++)\n",
    "\n",
    "Repeat until convergence or max_iter:\n",
    "    # Assignment\n",
    "    For i = 1..n:\n",
    "        c_i ← argmin_j ||x_i − μ_j||²\n",
    "\n",
    "    # Update\n",
    "    For j = 1..k:\n",
    "        μ_j ← mean({ x_i : c_i = j })\n",
    "Output: centroids {μ_j}, labels {c_i}, inertia J = ∑_i ||x_i − μ_{c_i}||²\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce468de",
   "metadata": {},
   "source": [
    "\n",
    "# Linear AdaBoost Classifier — Theory, Implementation, and Visualization\n",
    "\n",
    "**Date:** 2025-10-14 04:44\n",
    "\n",
    "This notebook is a complete, self-contained guide to **AdaBoost** for (binary) classification, presented as a *linear additive model* in function space. It includes:\n",
    "- A brief, math-first introduction with clean $$ ... $$ LaTeX blocks suitable for Jupyter\n",
    "- The AdaBoost algorithm derived from exponential loss\n",
    "- A from-scratch Python implementation using **decision stumps** as weak learners\n",
    "- Visualizations of decision boundaries on synthetic datasets\n",
    "- A comparison with `sklearn.ensemble.AdaBoostClassifier` (SAMME.R)\n",
    "- Practical tips: regularization, class imbalance, and diagnostics\n",
    "\n",
    "> **Notation.** We use training samples $(\\mathbf{x}_i, y_i)$ with labels $y_i \\in \\{-1, +1\\}$ and features $\\mathbf{x}_i \\in \\mathbb{R}^d$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e640d",
   "metadata": {},
   "source": [
    "\n",
    "As a formatting check, here's the requested Bayes theorem example using $$ ... $$ blocks in a single cell:\n",
    "\n",
    "$$ P(y \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x}\\mid y)\\,P(y)}{P(\\mathbf{x})} \\;\\propto\\; P(\\mathbf{x}\\mid y)\\,P(y). $$\n",
    "\n",
    "And an inline example: We start from **Bayes’ theorem** for a class label $y \\in \\{1,\\ldots,C\\}$ and a feature vector $\\mathbf{x} = (x_1,\\ldots,x_d)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b0231",
   "metadata": {},
   "source": [
    "\n",
    "## 1. AdaBoost as a Linear Additive Model in Function Space\n",
    "\n",
    "AdaBoost combines many **weak learners** $h_t(\\mathbf{x}) \\in \\{-1, +1\\}$ into a strong classifier:\n",
    "$$ F_T(\\mathbf{x}) = \\sum_{t=1}^{T} \\alpha_t\\, h_t(\\mathbf{x}). $$\n",
    "The final prediction is the sign:\n",
    "$$ \\hat{y}(\\mathbf{x}) = \\mathrm{sign}\\!\\left(F_T(\\mathbf{x})\\right). $$\n",
    "\n",
    "AdaBoost minimizes the **exponential loss**:\n",
    "$$ \\mathcal{L}(F) = \\sum_{i=1}^n \\exp\\!\\left(-y_i\\,F(\\mathbf{x}_i)\\right). $$\n",
    "\n",
    "At each round $t$, we fit a weak learner $h_t$ to the **weighted** sample distribution $D_t(i)$ and choose a weight $\\alpha_t$ to reduce the loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5fb046",
   "metadata": {},
   "source": [
    "\n",
    "### 1.1 Weighted error, learner weight, and distribution update\n",
    "\n",
    "Given weights $D_t(i)$ with $\\sum_i D_t(i)=1$, the **weighted error** of $h_t$ is\n",
    "$$ \\varepsilon_t = \\sum_{i=1}^{n} D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(\\mathbf{x}_i)\\}. $$\n",
    "\n",
    "If $\\varepsilon_t < \\tfrac{1}{2}$, we set\n",
    "$$ \\alpha_t = \\tfrac{1}{2}\\,\\ln\\!\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right). $$\n",
    "\n",
    "We then update the distribution\n",
    "$$ D_{t+1}(i) = \\frac{ D_t(i)\\,\\exp\\!\\left(-\\alpha_t\\,y_i\\,h_t(\\mathbf{x}_i)\\right) }{ Z_t }, $$\n",
    "where $Z_t$ is a normalization constant so that $\\sum_i D_{t+1}(i)=1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc765d",
   "metadata": {},
   "source": [
    "\n",
    "### 1.2 Interpretation\n",
    "\n",
    "- The model $F_T(\\mathbf{x})$ is **linear** in the basis functions $\\{h_t(\\cdot)\\}_{t=1}^T$.\n",
    "- Misclassified points get **upweighted**, forcing subsequent weak learners to focus on harder cases.\n",
    "- The sign of $F_T(\\mathbf{x})$ yields the class; $|F_T(\\mathbf{x})|$ is a **margin** proxy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd90529",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Pseudocode (Binary AdaBoost with Decision Stumps)\n",
    "\n",
    "**Input:** training set $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$ with $y_i \\in \\{-1, +1\\}$, number of rounds $T$.  \n",
    "**Initialize:** $D_1(i) = \\tfrac{1}{n}$ for all $i$.\n",
    "\n",
    "For $t = 1, \\ldots, T$:\n",
    "1. Train weak learner $h_t$ using weights $D_t$ (e.g., a 1D threshold per feature, pick best).\n",
    "2. Compute weighted error: $$ \\varepsilon_t = \\sum_i D_t(i)\\,\\mathbb{1}\\{y_i \\neq h_t(\\mathbf{x}_i)\\}. $$\n",
    "3. Set $$ \\alpha_t = \\tfrac{1}{2}\\ln\\!\\left(\\frac{1-\\varepsilon_t}{\\varepsilon_t}\\right). $$\n",
    "4. Update $$ D_{t+1}(i) \\propto D_t(i)\\,\\exp\\!\\left(-\\alpha_t y_i h_t(\\mathbf{x}_i)\\right). $$\n",
    "\n",
    "**Output:** $F_T(\\mathbf{x})=\\sum_{t=1}^T \\alpha_t h_t(\\mathbf{x})$, classifier $\\hat{y}(\\mathbf{x})=\\mathrm{sign}(F_T(\\mathbf{x}))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318e2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. From-scratch implementation of AdaBoost with decision stumps\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "@dataclass\n",
    "class DecisionStump:\n",
    "    feature: int = 0\n",
    "    threshold: float = 0.0\n",
    "    polarity: int = 1  # +1 or -1\n",
    "    def predict(self, X):\n",
    "        # X: (n_samples, n_features)\n",
    "        feature_values = X[:, self.feature]\n",
    "        preds = np.ones(X.shape[0], dtype=int)\n",
    "        if self.polarity == 1:\n",
    "            preds[feature_values < self.threshold] = -1\n",
    "        else:\n",
    "            preds[feature_values >= self.threshold] = -1\n",
    "        return preds\n",
    "\n",
    "class AdaBoostScratch:\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.alphas = []\n",
    "        self.stumps = []\n",
    "\n",
    "    def _best_stump(self, X, y, w):\n",
    "        n_samples, n_features = X.shape\n",
    "        best_stump = DecisionStump()\n",
    "        min_error = np.inf\n",
    "\n",
    "        for f in range(n_features):\n",
    "            values = np.unique(X[:, f])\n",
    "            if values.size == 1:\n",
    "                thresholds = values\n",
    "            else:\n",
    "                thresholds = (values[:-1] + values[1:]) / 2.0\n",
    "\n",
    "            for polarity in (+1, -1):\n",
    "                for thr in thresholds:\n",
    "                    stump = DecisionStump(feature=f, threshold=thr, polarity=polarity)\n",
    "                    preds = stump.predict(X)\n",
    "                    misclassified = (preds != y)\n",
    "                    error = np.dot(w, misclassified.astype(float))\n",
    "                    if error < min_error:\n",
    "                        min_error = error\n",
    "                        best_stump = stump\n",
    "        return best_stump, min_error\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Expect y in {-1, +1}\n",
    "        n_samples = X.shape[0]\n",
    "        w = np.ones(n_samples) / n_samples\n",
    "        self.alphas = []\n",
    "        self.stumps = []\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            stump, error = self._best_stump(X, y, w)\n",
    "            error = max(1e-10, min(0.4999999999, error))  # clamp\n",
    "            alpha = 0.5 * np.log((1 - error) / error)\n",
    "            alpha *= self.learning_rate  # shrinkage\n",
    "\n",
    "            preds = stump.predict(X)\n",
    "            w *= np.exp(-alpha * y * preds)\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.stumps.append(stump)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        F = np.zeros(X.shape[0])\n",
    "        for alpha, stump in zip(self.alphas, self.stumps):\n",
    "            F += alpha * stump.predict(X)\n",
    "        return F\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.decision_function(X)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Synthetic datasets and training\n",
    "\n",
    "from sklearn.datasets import make_classification, make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def to_pm1(y01):\n",
    "    return np.where(y01 == 1, 1, -1).astype(int)\n",
    "\n",
    "# Dataset A: linearly separable-ish\n",
    "X1, y1 = make_classification(n_samples=600, n_features=2, n_redundant=0, n_informative=2,\n",
    "                             n_clusters_per_class=1, class_sep=1.2, flip_y=0.05, random_state=0)\n",
    "y1_pm = to_pm1(y1)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1_pm, test_size=0.3, random_state=0)\n",
    "\n",
    "# Dataset B: moons (nonlinear structure)\n",
    "X2, y2 = make_moons(n_samples=600, noise=0.25, random_state=0)\n",
    "y2_pm = to_pm1(y2)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2_pm, test_size=0.3, random_state=0)\n",
    "\n",
    "# Scaling for stability across features\n",
    "scaler1 = StandardScaler().fit(X1_train)\n",
    "X1_train_s = scaler1.transform(X1_train)\n",
    "X1_test_s  = scaler1.transform(X1_test)\n",
    "\n",
    "scaler2 = StandardScaler().fit(X2_train)\n",
    "X2_train_s = scaler2.transform(X2_train)\n",
    "X2_test_s  = scaler2.transform(X2_test)\n",
    "\n",
    "# Train from-scratch AdaBoost\n",
    "model1 = AdaBoostScratch(n_estimators=50, learning_rate=1.0).fit(X1_train_s, y1_train)\n",
    "model2 = AdaBoostScratch(n_estimators=150, learning_rate=0.8).fit(X2_train_s, y2_train)\n",
    "\n",
    "pred1 = model1.predict(X1_test_s)\n",
    "pred2 = model2.predict(X2_test_s)\n",
    "\n",
    "print(\"Dataset A (linear-ish) accuracy:\", accuracy_score(y1_test, pred1))\n",
    "print(\"Dataset B (moons) accuracy:\", accuracy_score(y2_test, pred2))\n",
    "\n",
    "def prob_from_scores(scores):\n",
    "    return 1 / (1 + np.exp(-scores))\n",
    "\n",
    "auc1 = roc_auc_score((y1_test==1).astype(int), prob_from_scores(model1.decision_function(X1_test_s)))\n",
    "auc2 = roc_auc_score((y2_test==1).astype(int), prob_from_scores(model2.decision_function(X2_test_s)))\n",
    "print(\"AUC A:\", auc1, \" | AUC B:\", auc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1c1c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Visualization helpers: decision boundaries\n",
    "\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision boundary\", scaler=None, h=0.02):\n",
    "    if scaler is not None:\n",
    "        X_plot = scaler.transform(X)\n",
    "    else:\n",
    "        X_plot = X\n",
    "\n",
    "    x_min, x_max = X_plot[:, 0].min() - 1.0, X_plot[:, 0].max() + 1.0\n",
    "    y_min, y_max = X_plot[:, 1].min() - 1.0, X_plot[:, 1].max() + 1.0\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, levels=[-np.inf, 0, np.inf])\n",
    "    idx_pos = (y == 1)\n",
    "    idx_neg = (y == -1)\n",
    "    plt.scatter(X_plot[idx_pos, 0], X_plot[idx_pos, 1], s=20, label=\"+1\")\n",
    "    plt.scatter(X_plot[idx_neg, 0], X_plot[idx_neg, 1], s=20, marker=\"x\", label=\"-1\")\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"x1 (scaled)\" if scaler is not None else \"x1\")\n",
    "    plt.ylabel(\"x2 (scaled)\" if scaler is not None else \"x2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model1, X1_train, y1_train, title=\"AdaBoost (scratch) on Dataset A\", scaler=scaler1)\n",
    "plot_decision_boundary(model2, X2_train, y2_train, title=\"AdaBoost (scratch) on Dataset B (moons)\", scaler=scaler2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa8768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Compare with scikit-learn's AdaBoost (SAMME.R)\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "sk_model1 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    random_state=0\n",
    ").fit(X1_train_s, (y1_train==1).astype(int))\n",
    "\n",
    "sk_model2 = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.8,\n",
    "    algorithm=\"SAMME.R\",\n",
    "    random_state=0\n",
    ").fit(X2_train_s, (y2_train==1).astype(int))\n",
    "\n",
    "sk_pred1 = np.where(sk_model1.predict(X1_test_s)==1, 1, -1)\n",
    "sk_pred2 = np.where(sk_model2.predict(X2_test_s)==1, 1, -1)\n",
    "\n",
    "print(\"Sklearn - Dataset A accuracy:\", accuracy_score(y1_test, sk_pred1))\n",
    "print(\"Sklearn - Dataset B accuracy:\", accuracy_score(y2_test, sk_pred2))\n",
    "\n",
    "class SKWrapper:\n",
    "    def __init__(self, clf): self.clf = clf\n",
    "    def predict(self, X):\n",
    "        return np.where(self.clf.decision_function(X) >= 0, 1, -1)\n",
    "\n",
    "plot_decision_boundary(SKWrapper(sk_model1), X1_train, y1_train, title=\"sklearn AdaBoost on Dataset A\", scaler=scaler1)\n",
    "plot_decision_boundary(SKWrapper(sk_model2), X2_train, y2_train, title=\"sklearn AdaBoost on Dataset B (moons)\", scaler=scaler2)\n",
    "\n",
    "print(\"Confusion Matrix (A):\\n\", confusion_matrix(y1_test, sk_pred1))\n",
    "print(\"Confusion Matrix (B):\\n\", confusion_matrix(y2_test, sk_pred2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3138bd1",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Regularization, learning rate, and early stopping\n",
    "\n",
    "- **Learning rate (shrinkage):** use $\\nu \\in (0,1]$ to scale each $\\alpha_t \\leftarrow \\nu\\,\\alpha_t$ to reduce overfitting.\n",
    "- **Early stopping:** monitor validation error or margin distribution and stop when performance plateaus.\n",
    "- **Max depth of stumps/trees:** keep weak learners truly weak (e.g., depth-1 stumps) for the classic AdaBoost behavior.\n",
    "- **Class imbalance:** reweight initial $D_1(i)$ by class priors or use stratified sampling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8. Diagnostics: training margins and staged performance\n",
    "\n",
    "def compute_margins(model, X, y):\n",
    "    scores = model.decision_function(X)\n",
    "    return y * scores\n",
    "\n",
    "margins1 = compute_margins(model1, X1_train_s, y1_train)\n",
    "margins2 = compute_margins(model2, X2_train_s, y2_train)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(margins1, bins=30, alpha=0.7)\n",
    "plt.title(\"Margin distribution — Dataset A (scratch model)\")\n",
    "plt.xlabel(\"y * F(x)\"); plt.ylabel(\"count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(margins2, bins=30, alpha=0.7)\n",
    "plt.title(\"Margin distribution — Dataset B (scratch model)\")\n",
    "plt.xlabel(\"y * F(x)\"); plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65a0f5",
   "metadata": {},
   "source": [
    "\n",
    "## 9. References\n",
    "\n",
    "- Yoav Freund and Robert E. Schapire. *A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting.* Journal of Computer and System Sciences, 1997.\n",
    "- Trevor Hastie, Robert Tibshirani, Jerome Friedman. *The Elements of Statistical Learning*, 2nd ed., Ch. 10.\n",
    "- Schapire, R. E. (2013). *Explaining AdaBoost.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca6546",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10. Minimal \"API\" for reuse\n",
    "\n",
    "class LinearAdaBoostClassifier(AdaBoostScratch):\n",
    "    '''\n",
    "    A thin alias to highlight that F(x) is linear in weak learners.\n",
    "    Usage:\n",
    "        clf = LinearAdaBoostClassifier(n_estimators=100).fit(X, y_pm1)\n",
    "        yhat = clf.predict(X_test)\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "print(\"Ready: LinearAdaBoostClassifier\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

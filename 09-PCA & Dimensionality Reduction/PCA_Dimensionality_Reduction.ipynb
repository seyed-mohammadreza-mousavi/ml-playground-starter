{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd94edf",
   "metadata": {},
   "source": [
    "\n",
    "# üìâ Principal Component Analysis (PCA) & Dimensionality Reduction\n",
    "\n",
    "This notebook explains **Principal Component Analysis (PCA)** ‚Äî a fundamental technique in machine learning and data science for **dimensionality reduction** and **feature extraction**.\n",
    "\n",
    "We'll go through:\n",
    "1. Mathematical intuition  \n",
    "2. Step-by-step implementation from scratch  \n",
    "3. Visualization of variance and components  \n",
    "4. Comparison with scikit-learn's PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436346d6",
   "metadata": {},
   "source": [
    "\n",
    "## üßÆ 1. Mathematical Foundation\n",
    "\n",
    "PCA finds new axes (called **principal components**) that capture the **maximum variance** in the data.\n",
    "\n",
    "### Given:\n",
    "- A dataset \\( X \\in \\mathbb{R}^{n \times d} \\) with \\( n \\) samples and \\( d \\) features.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Standardize** the data (zero mean, unit variance):  \n",
    "   \\[\n",
    "   X_{std} = X - \\mu\n",
    "   \\]\n",
    "\n",
    "2. **Compute the covariance matrix**:  \n",
    "   \\[\n",
    "   \\Sigma = \\frac{1}{n-1} X_{std}^T X_{std}\n",
    "   \\]\n",
    "\n",
    "3. **Eigen decomposition** of covariance matrix:  \n",
    "   \\[\n",
    "   \\Sigma v = \\lambda v\n",
    "   \\]\n",
    "   - \\( v \\): eigenvectors (principal directions)  \n",
    "   - \\( \\lambda \\): eigenvalues (explained variance)\n",
    "\n",
    "4. **Sort eigenvectors** by descending eigenvalues.\n",
    "\n",
    "5. **Project** data onto top-k components:  \n",
    "   \\[\n",
    "   X_{proj} = X_{std} W_k\n",
    "   \\]\n",
    "\n",
    "Where \\( W_k \\) is the matrix of top-k eigenvectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f380dfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as SKPCA\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591c558",
   "metadata": {},
   "source": [
    "\n",
    "## ‚öôÔ∏è 2. PCA Implementation from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PCA_Scratch:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "\n",
    "    def fit(self, X):\n",
    "        # Standardize\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "\n",
    "        # Covariance matrix\n",
    "        cov = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "        # Eigen decomposition\n",
    "        eig_vals, eig_vecs = np.linalg.eigh(cov)\n",
    "\n",
    "        # Sort by eigenvalue descending\n",
    "        sorted_idx = np.argsort(eig_vals)[::-1]\n",
    "        eig_vals = eig_vals[sorted_idx]\n",
    "        eig_vecs = eig_vecs[:, sorted_idx]\n",
    "\n",
    "        # Store components and explained variance\n",
    "        self.components = eig_vecs[:, :self.n_components]\n",
    "        self.explained_variance = eig_vals[:self.n_components]\n",
    "        self.explained_variance_ratio = eig_vals[:self.n_components] / np.sum(eig_vals)\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_centered = X - self.mean\n",
    "        return np.dot(X_centered, self.components)\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592ccc5e",
   "metadata": {},
   "source": [
    "\n",
    "## üå∏ 3. Apply PCA on the Iris Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA (2 components)\n",
    "pca = PCA_Scratch(n_components=2)\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06139733",
   "metadata": {},
   "source": [
    "\n",
    "## üé® 4. Visualize the Reduced Data (2D Projection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea82026",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for label, color in zip(np.unique(y), ['red', 'green', 'blue']):\n",
    "    plt.scatter(X_pca[y==label, 0], X_pca[y==label, 1], label=iris.target_names[label], alpha=0.7, color=color)\n",
    "\n",
    "plt.title(\"PCA Projection of Iris Dataset (2 Components)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d4183",
   "metadata": {},
   "source": [
    "\n",
    "## üìä 5. Compare with scikit-learn's PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80fb5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sk_pca = SKPCA(n_components=2)\n",
    "X_pca_sk = sk_pca.fit_transform(X_std)\n",
    "\n",
    "print(\"Explained variance ratio (scikit-learn):\", sk_pca.explained_variance_ratio_)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for label, color in zip(np.unique(y), ['red', 'green', 'blue']):\n",
    "    plt.scatter(X_pca_sk[y==label, 0], X_pca_sk[y==label, 1], label=iris.target_names[label], alpha=0.7, color=color)\n",
    "\n",
    "plt.title(\"scikit-learn PCA Projection (2 Components)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f983bb",
   "metadata": {},
   "source": [
    "\n",
    "## üìà 6. Explained Variance Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de7bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio)+1), pca.explained_variance_ratio*100, alpha=0.7, color='steelblue')\n",
    "plt.ylabel(\"Explained Variance (%)\")\n",
    "plt.xlabel(\"Principal Components\")\n",
    "plt.title(\"Explained Variance by Components (PCA from Scratch)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb62568",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- PCA reduces dimensionality by projecting data onto orthogonal directions of **maximum variance**.  \n",
    "- The directions are found using **eigenvectors** of the covariance matrix.  \n",
    "- Eigenvalues represent the **amount of variance** captured by each principal component.  \n",
    "- PCA helps visualize high-dimensional data and improve computational efficiency.  \n",
    "\n",
    "---\n",
    "\n",
    "**Next project ‚Üí** *t-SNE & UMAP Visualization*\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d294409",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Discriminant Analysis (LDA): Theory, Implementation, and Visualization\n",
    "\n",
    "This notebook provides a **complete** overview of Linear Discriminant Analysis (LDA), including the math, derivation, implementation from scratch, and visualizations on toy and real datasets.\n",
    "\n",
    "We use display math with `$$ ... $$` and inline math with `$ ... $` so equations render correctly in Jupyter.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30493768",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Motivation and Probabilistic Background\n",
    "\n",
    "We start from **Bayes’ theorem** for a class label $y \\in \\{1,\\ldots,C\\}$ and a feature vector $\\mathbf{x} = (x_1,\\ldots,x_d)$:\n",
    "$$\n",
    "P(y \\mid \\mathbf{x}) = \\frac{P(\\mathbf{x}\\mid y)\\,P(y)}{P(\\mathbf{x})}\n",
    "\\;\\propto\\;\n",
    "P(\\mathbf{x}\\mid y)\\,P(y).\n",
    "$$\n",
    "\n",
    "In **Linear Discriminant Analysis (LDA)** we assume class-conditional **Gaussian** distributions with a **shared covariance** matrix:\n",
    "$$\n",
    "\\mathbf{x}\\mid y=c \\sim \\mathcal{N}(\\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}), \\qquad \\text{for all } c \\in \\{1,\\ldots,C\\}.\n",
    "$$\n",
    "\n",
    "Under this assumption, the **log-posterior** is (up to an additive constant independent of $c$) an **affine function** of $\\mathbf{x}$:\n",
    "$$\n",
    "\\log P(y=c\\mid \\mathbf{x})\n",
    "= \\mathbf{x}^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "-\\tfrac{1}{2}\\boldsymbol{\\mu}_c^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "+ \\log \\pi_c,\n",
    "$$\n",
    "where $\\pi_c = P(y=c)$ is the class prior.\n",
    "Hence the **decision rule** is linear in $\\mathbf{x}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b728d9",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Fisher's Linear Discriminant (Projection View)\n",
    "\n",
    "LDA can also be derived as a **supervised dimensionality reduction** technique. We seek a projection matrix $\\mathbf{W}\\in\\mathbb{R}^{d\\times m}$, $m\\le C-1$, that maximizes class separability in the projected space.\n",
    "\n",
    "Define the **class means** and **global mean**:\n",
    "$$\n",
    "\\boldsymbol{\\mu}_c = \\frac{1}{N_c}\\sum_{i: y_i=c}\\mathbf{x}_i, \\qquad\n",
    "\\boldsymbol{\\mu} = \\frac{1}{N}\\sum_{i=1}^N \\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "The **within-class scatter** and **between-class scatter** are\n",
    "$$\n",
    "\\mathbf{S}_W = \\sum_{c=1}^C \\sum_{i: y_i=c} (\\mathbf{x}_i-\\boldsymbol{\\mu}_c)(\\mathbf{x}_i-\\boldsymbol{\\mu}_c)^\\top,\n",
    "\\qquad\n",
    "\\mathbf{S}_B = \\sum_{c=1}^C N_c(\\boldsymbol{\\mu}_c-\\boldsymbol{\\mu})(\\boldsymbol{\\mu}_c-\\boldsymbol{\\mu})^\\top.\n",
    "$$\n",
    "\n",
    "Fisher's criterion maximizes\n",
    "$$\n",
    "J(\\mathbf{W}) = \\frac{\\det\\left(\\mathbf{W}^\\top \\mathbf{S}_B \\mathbf{W}\\right)}{\\det\\left(\\mathbf{W}^\\top \\mathbf{S}_W \\mathbf{W}\\right)}.\n",
    "$$\n",
    "\n",
    "The solution consists of the top $m$ **generalized eigenvectors** $\\{\\mathbf{w}_k\\}$ of\n",
    "$$\n",
    "\\mathbf{S}_B \\mathbf{w} = \\lambda \\mathbf{S}_W \\mathbf{w},\n",
    "$$\n",
    "i.e., the eigenvectors of $\\mathbf{S}_W^{-1}\\mathbf{S}_B$ (with appropriate regularization if $\\mathbf{S}_W$ is singular).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6354ba6a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Relation Between the Two Views\n",
    "\n",
    "Under the Gaussian **shared-covariance** assumption,\n",
    "$$\n",
    "\\boldsymbol{\\Sigma} = \\frac{1}{N-C}\\sum_{c=1}^C \\sum_{i: y_i=c} (\\mathbf{x}_i-\\boldsymbol{\\mu}_c)(\\mathbf{x}_i-\\boldsymbol{\\mu}_c)^\\top,\n",
    "$$\n",
    "LDA's **discriminant function**\n",
    "$$\n",
    "g_c(\\mathbf{x}) \\equiv \\mathbf{x}^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "-\\tfrac{1}{2}\\boldsymbol{\\mu}_c^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "+ \\log \\pi_c\n",
    "$$\n",
    "leads to **linear decision boundaries**. The **projection view** (Fisher LDA) and the **generative view** (Gaussian LDA) are consistent and often implemented together: use the Fisher directions for dimensionality reduction and the generative rule for classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e366950",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Assumptions, Regularization, and Practical Notes\n",
    "\n",
    "- **Shared covariance**: all classes share the same $\\boldsymbol{\\Sigma}$. When this is violated, **Quadratic Discriminant Analysis (QDA)** may be more appropriate.\n",
    "- **Regularization**: when $d$ is large vs $N$, $\\mathbf{S}_W$ or $\\boldsymbol{\\Sigma}$ can be ill-conditioned. One can use a ridge term: $\\boldsymbol{\\Sigma}_\\lambda = \\boldsymbol{\\Sigma} + \\lambda \\mathbf{I}$ with small $\\lambda>0$.\n",
    "- **Max number of LDA components**: the subspace dimension is at most $C-1$.\n",
    "- **Standardization**: it is common to standardize features before fitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "from sklearn.datasets import load_iris, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as SklearnLDA\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d756ae00",
   "metadata": {},
   "source": [
    "\n",
    "## 6. From-Scratch Implementation\n",
    "\n",
    "We implement two pieces:\n",
    "\n",
    "1. **Fisher directions** via the generalized eigenproblem of $\\mathbf{S}_W^{-1}\\mathbf{S}_B$ (for supervised dimensionality reduction).\n",
    "2. **Generative LDA classifier** using the shared-covariance Gaussian model with discriminant $g_c(\\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dd1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class LDAFromScratch:\n",
    "    n_components: Optional[int] = None\n",
    "    reg: float = 1e-6  # ridge for numerical stability\n",
    "    \n",
    "    # Learned parameters\n",
    "    classes_: np.ndarray = None\n",
    "    priors_: np.ndarray = None\n",
    "    means_: np.ndarray = None      # shape (C, d)\n",
    "    Sigma_: np.ndarray = None      # shared covariance (d, d)\n",
    "    W_: Optional[np.ndarray] = None  # projection matrix (d, m)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        C = len(self.classes_)\n",
    "        N, d = X.shape\n",
    "        \n",
    "        # Means per class and priors\n",
    "        self.means_ = np.vstack([X[y_idx == c].mean(axis=0) for c in range(C)])\n",
    "        counts = np.bincount(y_idx)\n",
    "        self.priors_ = counts / counts.sum()\n",
    "        \n",
    "        # Within-class scatter SW and between-class scatter SB\n",
    "        mu = X.mean(axis=0)\n",
    "        SW = np.zeros((d, d))\n",
    "        SB = np.zeros((d, d))\n",
    "        for c in range(C):\n",
    "            Xc = X[y_idx == c]\n",
    "            centered = Xc - self.means_[c]\n",
    "            SW += centered.T @ centered\n",
    "            mean_diff = (self.means_[c] - mu).reshape(-1, 1)\n",
    "            SB += counts[c] * (mean_diff @ mean_diff.T)\n",
    "        \n",
    "        # Shared covariance estimate (pooled)\n",
    "        self.Sigma_ = SW / (N - C)\n",
    "        # Regularization for numerical stability\n",
    "        self.Sigma_ = self.Sigma_ + self.reg * np.eye(d)\n",
    "        \n",
    "        # Fisher projection directions (optional)\n",
    "        m = self.n_components if self.n_components is not None else min(d, C - 1)\n",
    "        # Solve generalized eigenproblem via eig of inv(SW) SB (with regularization)\n",
    "        # For stability, solve linear system rather than explicit inverse\n",
    "        from numpy.linalg import solve, eig\n",
    "        A = solve(self.Sigma_, SB)  # equivalent to inv(SW) @ SB under Gaussian-LDA\n",
    "        eigvals, eigvecs = eig(A)\n",
    "        # Sort by descending eigenvalues (real part)\n",
    "        order = np.argsort(-eigvals.real)\n",
    "        W = eigvecs[:, order[:m]].real\n",
    "        # Orthonormalize columns via QR for numerical stability\n",
    "        Q, _ = np.linalg.qr(W)\n",
    "        self.W_ = Q[:, :m]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        if self.W_ is None:\n",
    "            raise ValueError(\"Model is not fitted with projection. Fit first.\")\n",
    "        return np.asarray(X, dtype=float) @ self.W_\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        # Precompute Sigma^{-1}\n",
    "        Sigma_inv = np.linalg.inv(self.Sigma_)\n",
    "        C = len(self.classes_)\n",
    "        scores = np.zeros((X.shape[0], C))\n",
    "        for c in range(C):\n",
    "            mu_c = self.means_[c]\n",
    "            term1 = X @ (Sigma_inv @ mu_c)\n",
    "            term2 = -0.5 * (mu_c.T @ Sigma_inv @ mu_c)\n",
    "            term3 = np.log(self.priors_[c] + 1e-12)\n",
    "            scores[:, c] = term1 + term2 + term3\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        scores = self.decision_function(X)\n",
    "        idx = scores.argmax(axis=1)\n",
    "        return self.classes_[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_regions_2d(model, X, y, title=\"Decision regions\", h=0.02, ax=None):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=0.25)\n",
    "    scatter = ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "    ax.set_title(title)\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335f32d",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Example 1 — Iris Dataset (3 classes)\n",
    "\n",
    "We fit our from-scratch LDA and compare with `scikit-learn`. We also visualize the data projected to $C-1=2$ dimensions via Fisher LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and split Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "# From-scratch LDA (2 components because C-1=2 for Iris)\n",
    "lda_fs = LDAFromScratch(n_components=2, reg=1e-4).fit(X_train, y_train)\n",
    "Z_train = lda_fs.transform(X_train)\n",
    "Z_test = lda_fs.transform(X_test)\n",
    "\n",
    "# Train a simple linear classifier in the projected space by reusing LDA's own discriminant\n",
    "# (prediction uses the generative discriminant already)\n",
    "y_pred_fs = lda_fs.predict(X_test)\n",
    "acc_fs = accuracy_score(y_test, y_pred_fs)\n",
    "\n",
    "print(\"From-scratch LDA accuracy (test):\", acc_fs)\n",
    "print(classification_report(y_test, y_pred_fs))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_fs))\n",
    "\n",
    "# Plot the 2D Fisher projection (no decision regions yet, since original LDA predicts in original space)\n",
    "plt.figure()\n",
    "plt.scatter(Z_train[:,0], Z_train[:,1], c=y_train, s=25, edgecolor='k')\n",
    "plt.title(\"Iris — Fisher LDA Projection (train)\")\n",
    "plt.xlabel(\"LD1\"); plt.ylabel(\"LD2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2eca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# scikit-learn LDA for comparison\n",
    "sk_lda = SklearnLDA(n_components=2)\n",
    "sk_lda.fit(X_train, y_train)\n",
    "y_pred_sk = sk_lda.predict(X_test)\n",
    "acc_sk = accuracy_score(y_test, y_pred_sk)\n",
    "print(\"sklearn LDA accuracy (test):\", acc_sk)\n",
    "print(classification_report(y_test, y_pred_sk))\n",
    "print(\"Confusion matrix (sklearn):\\n\", confusion_matrix(y_test, y_pred_sk))\n",
    "\n",
    "# Project with sklearn LDA\n",
    "Z_train_sk = sk_lda.transform(X_train)\n",
    "plt.figure()\n",
    "plt.scatter(Z_train_sk[:,0], Z_train_sk[:,1], c=y_train, s=25, edgecolor='k')\n",
    "plt.title(\"Iris — sklearn LDA Projection (train)\")\n",
    "plt.xlabel(\"LD1\"); plt.ylabel(\"LD2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c797a6d",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Example 2 — Toy Blobs (2D) with Decision Regions\n",
    "\n",
    "We generate a 3-class 2D dataset and visualize the **linear decision boundaries** produced by LDA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5242e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Xb, yb = make_blobs(n_samples=600, centers=3, cluster_std=2.0, random_state=7)\n",
    "Xb = StandardScaler().fit_transform(Xb)\n",
    "Xb_tr, Xb_te, yb_tr, yb_te = train_test_split(Xb, yb, test_size=0.3, stratify=yb, random_state=7)\n",
    "\n",
    "lda_blobs = LDAFromScratch(n_components=2, reg=1e-4).fit(Xb_tr, yb_tr)\n",
    "yp = lda_blobs.predict(Xb_te)\n",
    "print(\"Blobs — from-scratch LDA accuracy:\", accuracy_score(yb_te, yp))\n",
    "\n",
    "# Since the data is already 2D, we can plot the decision regions directly\n",
    "fig, ax = plt.subplots()\n",
    "plot_decision_regions_2d(lda_blobs, Xb_tr, yb_tr, title=\"LDA Decision Regions (train)\", ax=ax)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c3af1",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Explicit Decision Rule\n",
    "\n",
    "Given estimates of $\\boldsymbol{\\mu}_c$, $\\boldsymbol{\\Sigma}$, and $\\pi_c$, the LDA discriminant is:\n",
    "$$\n",
    "g_c(\\mathbf{x})\n",
    "= \\mathbf{x}^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "-\\tfrac{1}{2}\\boldsymbol{\\mu}_c^\\top \\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_c\n",
    "+ \\log \\pi_c.\n",
    "$$\n",
    "We predict $\\hat{y}=\\arg\\max_c g_c(\\mathbf{x})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafa757",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Algorithmic Summary (Pseudocode)\n",
    "\n",
    "**Fit** on data matrix $X\\in\\mathbb{R}^{N\\times d}$ and labels $y$:\n",
    "1. Compute class means $\\boldsymbol{\\mu}_c$ and priors $\\pi_c$.\n",
    "2. Compute pooled covariance (within-class scatter) $\\boldsymbol{\\Sigma}$ and regularize $\\boldsymbol{\\Sigma}_\\lambda=\\boldsymbol{\\Sigma}+\\lambda\\mathbf{I}$.\n",
    "3. (Optional) Solve generalized eigenproblem for Fisher projection $\\mathbf{S}_B\\mathbf{w}=\\lambda\\mathbf{S}_W\\mathbf{w}$ to obtain up to $C-1$ directions.\n",
    "4. Store $\\{\\boldsymbol{\\mu}_c\\}$, $\\boldsymbol{\\Sigma}_\\lambda$, $\\{\\pi_c\\}$ (and projection $\\mathbf{W}$ if needed).\n",
    "\n",
    "**Predict** on a new $\\mathbf{x}$:\n",
    "1. Compute $g_c(\\mathbf{x})$ for each class using the formula above.\n",
    "2. Return the class with the largest score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcad2b2",
   "metadata": {},
   "source": [
    "\n",
    "## 11. When LDA Works Well (and When It Doesn't)\n",
    "\n",
    "LDA shines when:\n",
    "- Classes are approximately Gaussian with similar covariances.\n",
    "- Sample size is moderate and you need a low-dimensional **supervised** projection (up to $C-1$).\n",
    "\n",
    "Consider alternatives when:\n",
    "- Covariances differ substantially across classes (use QDA).\n",
    "- Distributions are multi-modal or not well-approximated by Gaussians (try discriminative models or mixtures).\n",
    "- $d \\gg N$: use stronger regularization or prior dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf35306",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Reproducibility\n",
    "\n",
    "All random seeds are fixed where relevant for reproducible splits. You can re-run the notebook end-to-end to reproduce the figures and metrics.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

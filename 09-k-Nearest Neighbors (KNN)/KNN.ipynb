{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08c0d9a",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 K-Nearest Neighbors (KNN) from Scratch\n",
    "\n",
    "This notebook demonstrates **K-Nearest Neighbors (KNN)** — one of the simplest yet powerful non-parametric algorithms in machine learning.  \n",
    "We’ll go through:\n",
    "\n",
    "1. Mathematical intuition  \n",
    "2. Implementation from scratch using NumPy  \n",
    "3. Visualization on synthetic data  \n",
    "4. Comparison with scikit-learn implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3fa7a",
   "metadata": {},
   "source": [
    "\n",
    "## 📘 1. Mathematical Intuition\n",
    "\n",
    "The **KNN algorithm** classifies a new sample based on the majority label of its **K nearest points** in the feature space.\n",
    "\n",
    "### Formula for Distance\n",
    "For two points $x_i$ and $x_j$ in a $d$-dimensional space:\n",
    "\n",
    "$$\n",
    "d(x_i, x_j) = \\sqrt{\\sum_{k=1}^{d} (x_{ik} - x_{jk})^2}\n",
    "$$\n",
    "\n",
    "### Decision Rule\n",
    "For a query point $x_q$:\n",
    "1. Compute distances to all training samples.\n",
    "2. Select the **K smallest distances**.\n",
    "3. Assign the label with the majority vote among those K neighbors.\n",
    "\n",
    "KNN can also be used for regression (average of K neighbors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70c085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643994a3",
   "metadata": {},
   "source": [
    "\n",
    "## ⚙️ 2. Implementation from Scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da35377",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        # Compute distances\n",
    "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        # Get K nearest labels\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_labels = [self.y_train[i] for i in k_indices]\n",
    "        # Majority vote\n",
    "        most_common = Counter(k_labels).most_common(1)\n",
    "        return most_common[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b82e76e",
   "metadata": {},
   "source": [
    "\n",
    "## 🧩 3. Generate and Visualize Data\n",
    "We'll create a 2D dataset to visualize decision boundaries easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b06cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, \n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"Synthetic 2D Dataset\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9be39",
   "metadata": {},
   "source": [
    "\n",
    "## 🔍 4. Train and Evaluate Our KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f7dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "knn = KNN(k=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy (Scratch KNN): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222f435b",
   "metadata": {},
   "source": [
    "\n",
    "## 🎨 5. Visualize Decision Boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary (KNN)\"):\n",
    "    h = 0.1\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap='coolwarm')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(knn, X, y, title=\"Decision Boundary - KNN (from Scratch)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2f7bc",
   "metadata": {},
   "source": [
    "\n",
    "## 🧮 6. Compare with scikit-learn Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b098516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sk_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "sk_knn.fit(X_train, y_train)\n",
    "y_pred_lib = sk_knn.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy (scikit-learn KNN): {accuracy_score(y_test, y_pred_lib):.3f}\")\n",
    "plot_decision_boundary(sk_knn, X, y, title=\"Decision Boundary - KNN (scikit-learn)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa7d95",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- KNN is a **non-parametric**, **instance-based** learning algorithm.\n",
    "- It uses **distance metrics** to classify new points.\n",
    "- Works well for **low-dimensional data**, but scales poorly with large datasets.\n",
    "- No training phase (lazy learning), but prediction is **computationally expensive**.\n",
    "\n",
    "---\n",
    "\n",
    "**You learned:**\n",
    "- The math behind KNN  \n",
    "- Implementation from scratch  \n",
    "- Visualization of decision boundaries  \n",
    "- Comparison with scikit-learn’s version\n",
    "\n",
    "> Continue to next project → **Naive Bayes Classifier**\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

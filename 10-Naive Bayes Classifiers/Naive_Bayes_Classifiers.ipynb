{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c3680ad",
   "metadata": {},
   "source": [
    "\n",
    "# Naive Bayes Classifiers — Theory, Implementation, and Visualization\n",
    "\n",
    "This notebook gives a practical, end‑to‑end tour of Naive Bayes classifiers.  \n",
    "It includes the math, from‑scratch implementations (Gaussian, Multinomial, Bernoulli), comparisons with scikit‑learn, evaluation, and visualizations.\n",
    "\n",
    "**Contents**\n",
    "1. Intuition & Probabilistic Foundations  \n",
    "2. Multinomial, Bernoulli, and Gaussian Naive Bayes — assumptions and use‑cases  \n",
    "3. From‑scratch implementations (with Laplace smoothing)  \n",
    "4. Synthetic & real datasets (Iris, Digits)  \n",
    "5. Metrics: accuracy, precision, recall, F1, confusion matrix, ROC‑AUC (binary)  \n",
    "6. Decision boundary visualizations for 2D cases  \n",
    "7. Practical tips, pitfalls, and when Naive Bayes shines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec1be6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Intuition & Probabilistic Foundations\n",
    "\n",
    "We start from **Bayes’ theorem** for a class label \\( y \\in \\{1,\\dots,C\\} \\) and feature vector \\( \\mathbf{x}=(x_1,\\dots,x_d) \\):\n",
    "\\[\n",
    "P(y \\mid \\mathbf{x})=\\frac{P(\\mathbf{x}\\mid y)\\,P(y)}{P(\\mathbf{x})} \\propto P(\\mathbf{x}\\mid y)\\,P(y).\n",
    "\\]\n",
    "\n",
    "Naive Bayes assumes **conditional independence** of features given the class:\n",
    "\\[\n",
    "P(\\mathbf{x}\\mid y)=\\prod_{j=1}^{d} P(x_j\\mid y).\n",
    "\\]\n",
    "\n",
    "We predict the **maximum a posteriori** (MAP) class:\n",
    "\\[\n",
    "\\hat{y}=\\arg\\max_y \\ \\log P(y) + \\sum_{j=1}^{d}\\log P(x_j\\mid y).\n",
    "\\]\n",
    "\n",
    "Taking logs avoids underflow and turns products into sums.\n",
    "\n",
    "### Class prior\n",
    "\\[\n",
    "\\hat{P}(y=c)=\\frac{N_c}{N},\n",
    "\\]\n",
    "where \\(N_c\\) is the number of samples in class \\(c\\) and \\(N\\) is the total number of samples.\n",
    "\n",
    "### Likelihood models\n",
    "Different Naive Bayes variants specify \\(P(x_j\\mid y)\\) differently.\n",
    "\n",
    "**Multinomial NB (counts/frequencies, e.g., bag‑of‑words):**\n",
    "\\[\n",
    "P(\\mathbf{x}\\mid y=c)=\\frac{\\left(\\sum_j x_j\\right)!}{\\prod_j x_j!}\\prod_{j=1}^d \\theta_{c j}^{\\,x_j},\\quad\n",
    "\\theta_{c j}=\\frac{n_{c j}+\\alpha}{\\sum_{k=1}^d (n_{c k}+\\alpha)}.\n",
    "\\]\n",
    "Here \\(n_{c j}\\) is the total count of feature \\(j\\) in class \\(c\\), and \\(\\alpha\\ge 0\\) is Laplace (add‑\\(\\alpha\\)) smoothing.\n",
    "\n",
    "**Bernoulli NB (binary features, presence/absence):**\n",
    "\\[\n",
    "P(\\mathbf{x}\\mid y=c)=\\prod_{j=1}^{d} \\theta_{c j}^{x_j}\\,(1-\\theta_{c j})^{1-x_j},\\quad\n",
    "\\theta_{c j}=\\frac{n_{c j}+\\alpha}{N_c+2\\alpha},\n",
    "\\]\n",
    "where \\(n_{c j}\\) is the number of samples of class \\(c\\) where feature \\(j\\)=1.\n",
    "\n",
    "**Gaussian NB (continuous features):**\n",
    "\\[\n",
    "P(x_j\\mid y=c)=\\mathcal{N}(x_j;\\,\\mu_{c j},\\sigma^2_{c j})=\\frac{1}{\\sqrt{2\\pi \\sigma^2_{c j}}}\\exp\\!\\left(-\\frac{(x_j-\\mu_{c j})^2}{2\\sigma^2_{c j}}\\right).\n",
    "\\]\n",
    "We estimate \\(\\mu_{c j}\\) and \\(\\sigma^2_{c j}\\) from the data of class \\(c\\).\n",
    "\n",
    "### Decision rule in log‑space\n",
    "- **Multinomial:** \\(\\log P(y=c) + \\sum_j x_j \\log \\theta_{c j} + \\text{const}\\)  \n",
    "- **Bernoulli:** \\(\\log P(y=c) + \\sum_j \\big[x_j \\log \\theta_{c j} + (1-x_j)\\log(1-\\theta_{c j})\\big]\\)  \n",
    "- **Gaussian:** \\(\\log P(y=c) - \\tfrac{1}{2}\\sum_j\\big(\\log 2\\pi\\sigma^2_{c j} + \\frac{(x_j-\\mu_{c j})^2}{\\sigma^2_{c j}}\\big)\\)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816039a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Imports & Utility\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_digits, make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import MinMaxScaler, Binarizer\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "# Ensure reproducibility\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d428479",
   "metadata": {},
   "source": [
    "\n",
    "## 2) From‑Scratch Implementations\n",
    "\n",
    "We implement minimal, numerically stable versions of Gaussian, Multinomial, and Bernoulli Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7e1bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianNB_Scratch:\n",
    "    def __init__(self, var_smoothing=1e-9):\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.classes_ = None\n",
    "        self.theta_ = None  # means (n_classes, n_features)\n",
    "        self.var_ = None    # variances (n_classes, n_features)\n",
    "        self.class_count_ = None\n",
    "        self.class_prior_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        self.theta_ = np.zeros((n_classes, n_features))\n",
    "        self.var_ = np.zeros((n_classes, n_features))\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=float)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            Xc = X[y == c]\n",
    "            self.class_count_[i] = Xc.shape[0]\n",
    "            self.theta_[i] = Xc.mean(axis=0)\n",
    "            # Variance with small smoothing to avoid division by zero\n",
    "            self.var_[i] = Xc.var(axis=0) + self.var_smoothing\n",
    "        self.class_prior_ = self.class_count_ / self.class_count_.sum()\n",
    "        return self\n",
    "\n",
    "    def _joint_log_likelihood(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        n_samples, n_features = X.shape\n",
    "        n_classes = len(self.classes_)\n",
    "        jll = np.zeros((n_samples, n_classes))\n",
    "        for i in range(n_classes):\n",
    "            mu = self.theta_[i]\n",
    "            var = self.var_[i]\n",
    "            # log-likelihood per feature under Gaussian\n",
    "            log_prob = -0.5 * (np.log(2.0*np.pi*var) + ((X - mu) ** 2) / var).sum(axis=1)\n",
    "            jll[:, i] = np.log(self.class_prior_[i]) + log_prob\n",
    "        return jll\n",
    "\n",
    "    def predict(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        jll = self._joint_log_likelihood(X)\n",
    "        # log-sum-exp normalization\n",
    "        a = jll.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(jll - a)\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51210015",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultinomialNB_Scratch:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = float(alpha)\n",
    "        self.classes_ = None\n",
    "        self.class_count_ = None\n",
    "        self.feature_count_ = None  # counts per class-feature\n",
    "        self.feature_log_prob_ = None\n",
    "        self.class_log_prior_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        if (X < 0).any():\n",
    "            raise ValueError(\"MultinomialNB requires nonnegative features (counts/frequencies).\")\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=float)\n",
    "        self.feature_count_ = np.zeros((n_classes, n_features), dtype=float)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            Xc = X[y == c]\n",
    "            self.class_count_[i] = Xc.shape[0]\n",
    "            self.feature_count_[i] = Xc.sum(axis=0)\n",
    "        # Laplace smoothing\n",
    "        smoothed_fc = self.feature_count_ + self.alpha\n",
    "        smoothed_cc = smoothed_fc.sum(axis=1, keepdims=True)\n",
    "        self.feature_log_prob_ = np.log(smoothed_fc) - np.log(smoothed_cc)\n",
    "        self.class_log_prior_ = np.log(self.class_count_) - np.log(self.class_count_.sum())\n",
    "        return self\n",
    "\n",
    "    def _jll(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        return self.class_log_prior_ + X @ self.feature_log_prob_.T\n",
    "\n",
    "    def predict(self, X):\n",
    "        jll = self._jll(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        jll = self._jll(X)\n",
    "        a = jll.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(jll - a)\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f88838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BernoulliNB_Scratch:\n",
    "    def __init__(self, alpha=1.0, binarize_threshold=0.0):\n",
    "        self.alpha = float(alpha)\n",
    "        self.binarize_threshold = binarize_threshold\n",
    "        self.classes_ = None\n",
    "        self.class_count_ = None\n",
    "        self.feature_prob_ = None  # theta_{cj}\n",
    "        self.class_log_prior_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        y = np.asarray(y)\n",
    "        # Binarize\n",
    "        Xb = (X > self.binarize_threshold).astype(float)\n",
    "        self.classes_, y_idx = np.unique(y, return_inverse=True)\n",
    "        n_classes = len(self.classes_)\n",
    "        n_features = X.shape[1]\n",
    "        self.class_count_ = np.zeros(n_classes, dtype=float)\n",
    "        feature_on = np.zeros((n_classes, n_features), dtype=float)\n",
    "        for i, c in enumerate(self.classes_):\n",
    "            Xc = Xb[y == c]\n",
    "            self.class_count_[i] = Xc.shape[0]\n",
    "            feature_on[i] = Xc.sum(axis=0)\n",
    "        # Laplace smoothing for Bernoulli\n",
    "        self.feature_prob_ = (feature_on + self.alpha) / (self.class_count_.reshape(-1,1) + 2*self.alpha)\n",
    "        self.class_log_prior_ = np.log(self.class_count_) - np.log(self.class_count_.sum())\n",
    "        return self\n",
    "\n",
    "    def _jll(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        Xb = (X > self.binarize_threshold).astype(float)\n",
    "        # log p(x|y) = sum_j [ x_j log theta + (1-x_j) log (1-theta) ]\n",
    "        log_theta = np.log(self.feature_prob_)\n",
    "        log_1_minus = np.log(1 - self.feature_prob_)\n",
    "        jll = []\n",
    "        for i in range(len(self.classes_)):\n",
    "            ll = (Xb * log_theta[i] + (1 - Xb) * log_1_minus[i]).sum(axis=1)\n",
    "            jll.append(self.class_log_prior_[i] + ll)\n",
    "        return np.vstack(jll).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        jll = self._jll(X)\n",
    "        return self.classes_[np.argmax(jll, axis=1)]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        jll = self._jll(X)\n",
    "        a = jll.max(axis=1, keepdims=True)\n",
    "        probs = np.exp(jll - a)\n",
    "        probs /= probs.sum(axis=1, keepdims=True)\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5499ea7",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Experiments on Real and Synthetic Data\n",
    "\n",
    "We demonstrate:\n",
    "- **Gaussian NB** on Iris (continuous features),\n",
    "- **Multinomial NB** on Digits (nonnegative counts/intensities; we rescale to integer‑like counts),\n",
    "- **Bernoulli NB** on binarized Digits,\n",
    "- Decision boundaries on synthetic 2D data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a612b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iris — Gaussian NB (from-scratch vs scikit-learn)\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "gnb_s = GaussianNB_Scratch().fit(X_train, y_train)\n",
    "y_pred_s = gnb_s.predict(X_test)\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, y_train)\n",
    "y_pred_lib = gnb.predict(X_test)\n",
    "\n",
    "acc_s = accuracy_score(y_test, y_pred_s)\n",
    "acc_lib = accuracy_score(y_test, y_pred_lib)\n",
    "print(\"Iris — GaussianNB Scratch accuracy:\", acc_s)\n",
    "print(\"Iris — GaussianNB (sklearn) accuracy:\", acc_lib)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lib)\n",
    "print(\"Confusion matrix (sklearn GaussianNB):\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Digits — Multinomial NB (counts-like features)\n",
    "digits = load_digits()\n",
    "Xd, yd = digits.data, digits.target\n",
    "\n",
    "# Scale to [0, 1], then multiply to get pseudo-counts (integers) suitable for Multinomial\n",
    "scaler = MinMaxScaler()\n",
    "Xd01 = scaler.fit_transform(Xd)\n",
    "Xd_counts = np.round(Xd01 * 10.0)  # small counts to keep it simple\n",
    "\n",
    "Xtr, Xte, ytr, yte = train_test_split(Xd_counts, yd, test_size=0.3, random_state=42, stratify=yd)\n",
    "\n",
    "mnb_s = MultinomialNB_Scratch(alpha=1.0).fit(Xtr, ytr)\n",
    "yp_s = mnb_s.predict(Xte)\n",
    "\n",
    "mnb = MultinomialNB(alpha=1.0).fit(Xtr, ytr)\n",
    "yp_lib = mnb.predict(Xte)\n",
    "\n",
    "print(\"Digits — MultinomialNB Scratch accuracy:\", accuracy_score(yte, yp_s))\n",
    "print(\"Digits — MultinomialNB (sklearn) accuracy:\", accuracy_score(yte, yp_lib))\n",
    "print(\"Confusion matrix (sklearn MultinomialNB):\\n\", confusion_matrix(yte, yp_lib))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ed9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Digits — Bernoulli NB (binarized pixels)\n",
    "bin_thresh = 0.5\n",
    "binarizer = Binarizer(threshold=bin_thresh)\n",
    "Xd_bin = binarizer.fit_transform(Xd01)\n",
    "\n",
    "Xtr_b, Xte_b, ytr_b, yte_b = train_test_split(Xd_bin, yd, test_size=0.3, random_state=42, stratify=yd)\n",
    "\n",
    "bnb_s = BernoulliNB_Scratch(alpha=1.0, binarize_threshold=0.0).fit(Xtr_b, ytr_b)\n",
    "ypb_s = bnb_s.predict(Xte_b)\n",
    "\n",
    "bnb = BernoulliNB(alpha=1.0).fit(Xtr_b, ytr_b)\n",
    "ypb_lib = bnb.predict(Xte_b)\n",
    "\n",
    "print(\"Digits — BernoulliNB Scratch accuracy:\", accuracy_score(yte_b, ypb_s))\n",
    "print(\"Digits — BernoulliNB (sklearn) accuracy:\", accuracy_score(yte_b, ypb_lib))\n",
    "print(\"Confusion matrix (sklearn BernoulliNB):\\n\", confusion_matrix(yte_b, ypb_lib))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1279d7a3",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Visualizations\n",
    "\n",
    "We’ll plot decision boundaries for 2D synthetic datasets to build intuition.  \n",
    "(Each chart is a separate figure, uses matplotlib only, and does not set explicit colors.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper for decision boundary plots\n",
    "def plot_decision_boundary(model, X, y, title):\n",
    "    x_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\n",
    "    y_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
    "                         np.linspace(y_min, y_max, 300))\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = model.predict(grid).reshape(xx.shape)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=12, edgecolor=\"k\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
    "    plt.show()\n",
    "\n",
    "# Gaussian NB on blobs\n",
    "Xb, yb = make_blobs(n_samples=600, centers=3, cluster_std=1.3, random_state=42)\n",
    "Xtr_b2, Xte_b2, ytr_b2, yte_b2 = train_test_split(Xb, yb, test_size=0.3, random_state=42, stratify=yb)\n",
    "\n",
    "gnb_vis = GaussianNB_Scratch().fit(Xtr_b2, ytr_b2)\n",
    "plot_decision_boundary(gnb_vis, Xte_b2, yte_b2, \"Gaussian NB — Decision Boundary on Blobs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9375b91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Multinomial & Bernoulli demo on nonnegative 2D data\n",
    "# We'll synthesize nonnegative features for illustration.\n",
    "X_pos, y_pos = make_classification(n_samples=600, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                   n_clusters_per_class=1, random_state=7)\n",
    "X_pos = MinMaxScaler().fit_transform(X_pos) * 10.0  # scale to nonnegative counts-like\n",
    "Xtr_p, Xte_p, ytr_p, yte_p = train_test_split(X_pos, y_pos, test_size=0.3, random_state=42, stratify=y_pos)\n",
    "\n",
    "mnb_vis = MultinomialNB_Scratch(alpha=1.0).fit(Xtr_p, ytr_p)\n",
    "plot_decision_boundary(mnb_vis, Xte_p, yte_p, \"Multinomial NB — Decision Boundary on Nonnegative 2D Data\")\n",
    "\n",
    "# Bernoulli (binarize features)\n",
    "Xb2 = (X_pos > 5.0).astype(float)\n",
    "Xtr_bb, Xte_bb, ytr_bb, yte_bb = train_test_split(Xb2, y_pos, test_size=0.3, random_state=42, stratify=y_pos)\n",
    "\n",
    "bnb_vis = BernoulliNB_Scratch(alpha=1.0, binarize_threshold=0.5).fit(Xtr_bb, ytr_bb)\n",
    "plot_decision_boundary(bnb_vis, Xte_bb, yte_bb, \"Bernoulli NB — Decision Boundary on Binarized 2D Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24edf1da",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Metrics & Analysis\n",
    "\n",
    "We’ll compute accuracy, precision, recall, and F1.  \n",
    "For binary problems we’ll also show ROC‑AUC and a ROC curve example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd331de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Binary ROC example using a 2-class subset of Iris\n",
    "mask = y < 2  # classes 0 and 1\n",
    "X_bin, y_bin = X[mask], y[mask]\n",
    "\n",
    "Xtr_bi, Xte_bi, ytr_bi, yte_bi = train_test_split(X_bin, y_bin, test_size=0.3, random_state=42, stratify=y_bin)\n",
    "\n",
    "gnb_bin = GaussianNB_Scratch().fit(Xtr_bi, ytr_bi)\n",
    "proba = gnb_bin.predict_proba(Xte_bi)[:, 1]\n",
    "yp_bin = gnb_bin.predict(Xte_bi)\n",
    "\n",
    "acc = accuracy_score(yte_bi, yp_bin)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(yte_bi, yp_bin, average=\"binary\", zero_division=0)\n",
    "auc = roc_auc_score(yte_bi, proba)\n",
    "cm = confusion_matrix(yte_bi, yp_bin)\n",
    "\n",
    "print(\"Binary Iris — GaussianNB Scratch\")\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec, \"Recall:\", rec, \"F1:\", f1)\n",
    "print(\"ROC-AUC:\", auc)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thr = roc_curve(yte_bi, proba)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "plt.title(\"ROC Curve — Gaussian NB (Binary Iris)\")\n",
    "plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44f8543",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Practical Tips & Pitfalls\n",
    "\n",
    "- **When to use which variant:**  \n",
    "  Multinomial for count data (e.g., bag‑of‑words), Bernoulli for binary features (presence/absence), Gaussian for continuous features.\n",
    "- **Laplace/Add‑\\(\\alpha\\) smoothing** prevents zero probabilities when a feature never appears with a class.\n",
    "- **Feature scaling:** Gaussian NB does **not** require standardization (it models feature‑wise variances), but extreme scaling differences may impact numerical stability.\n",
    "- **Correlated features:** The naive conditional‑independence assumption is often violated; NB can still perform well, but heavy correlation can degrade performance.\n",
    "- **Imbalanced classes:** Adjust priors or use class‑balanced training; evaluate with precision/recall and ROC‑AUC.\n",
    "- **Text classification:** Convert text to **counts** or **TF‑IDF** (for Multinomial NB). (This notebook avoids external downloads for offline reproducibility.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e03b86d",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Summary\n",
    "\n",
    "We derived Naive Bayes from Bayes’ theorem, implemented three variants from scratch with smoothing and log‑space computation, validated them on Iris and Digits, visualized decision boundaries, and compared to scikit‑learn baselines. Naive Bayes is simple, fast, and surprisingly strong when its generative assumptions align reasonably with the data.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}